{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data, computing basic statistics and constructing simple models are essential steps for data science practice.you will use clinical data as raw input to perform **Heart Failure Prediction**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../project_1/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../project_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to `TRAIN_DATA_PATH`. There are three CSV files which will be the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls $TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**events.csv**\n",
    "\n",
    "The data provided in *events.csv* are event sequences. Each line of this file consists of a tuple with the format *(pid, event_id, vid, value)*. \n",
    "\n",
    "For example, \n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,DIAG_414,0,1\n",
    "33,DIAG_427,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50812,1,1\n",
    "33,DIAG_425,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "- **pid**: De-identified patient identier. For example, the patient in the example above has pid 33. \n",
    "- **event_id**: Clinical event identifier. For example, DIAG_244 means the patient was diagnosed of disease with ICD9 code [244](http://www.icd9data.com/2013/Volume1/240-279/240-246/244/244.htm); LAB_50971 means that the laboratory test with code 50971 was conducted on the patient; and DRUG_0 means that a drug with code 0 was prescribed to the patient. Corresponding lab (drug) names can be found in `{DATA_PATH}/lab_list.txt` (`{DATA_PATH}/drug_list.txt`).\n",
    "- **vid**: Visit identifier. For example, the patient has two visits in total. Note that vid is ordinal. That is, visits with bigger vid occour after that with smaller vid.\n",
    "- **value**: Contains the value associated to an event (always 1 in the synthesized dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hf_events.csv**\n",
    "\n",
    "The data provided in *hf_events.csv* contains pid of patients who have been diagnosed with heart failure (i.e., DIAG_398, DIAG_402, DIAG_404, DIAG_428) in at least one visit. They are in the form of a tuple with the format *(pid, vid, label)*. For example,\n",
    "\n",
    "```\n",
    "156,0,1\n",
    "181,1,1\n",
    "```\n",
    "\n",
    "The vid indicates the index of the first visit with heart failure of that patient and a label of 1 indicates the presence of heart failure. **Note that only patients with heart failure are included in this file. Patients who are not mentioned in this file have never been diagnosed with heart failure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**event_feature_map.csv**\n",
    "\n",
    "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Descriptive Statistics\n",
    "\n",
    "Before starting analytic modeling, it is a good practice to get descriptive statistics of the input raw data. In this question, you need to write code that computes various metrics on the data described previously. A skeleton code is provided to you as a starting point.\n",
    "\n",
    "The definition of terms used in the result table are described below:\n",
    "\n",
    "- **Event count**: Number of events recorded for a given patient.\n",
    "- **Encounter count**: Number of visits recorded for a given patient.\n",
    "\n",
    "Note that every line in the input file is an event, while each visit consists of multiple events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code cell to implement the required statistics.**\n",
    "\n",
    "Please be aware that **you are NOT allowed to change the filename and any existing function declarations.** Only `numpy`, `scipy`, `scikit-learn`, `pandas` and other built-in modules of python will be available for you to use. The use of `pandas` library is suggested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "\n",
    "    '''\n",
    "    Read the events.csv and hf_events.csv files. \n",
    "    Variables returned from this function are passed as input to the metric functions.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "    \n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "\n",
    "    return events, hf\n",
    "\n",
    "def event_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the event count metrics.\n",
    "    \n",
    "    Event count is defined as the number of events recorded for a given patient.\n",
    "    '''\n",
    "    \n",
    "    avg_hf_event_count = None\n",
    "    max_hf_event_count = None\n",
    "    min_hf_event_count = None\n",
    "    avg_norm_event_count = None\n",
    "    max_norm_event_count = None\n",
    "    min_norm_event_count = None\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
    "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
    "\n",
    "def encounter_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the encounter count metrics.\n",
    "    \n",
    "    Encounter count is defined as the number of visits recorded for a given patient. \n",
    "    '''\n",
    "    \n",
    "    avg_hf_encounter_count = None\n",
    "    max_hf_encounter_count = None\n",
    "    min_hf_encounter_count = None\n",
    "    avg_norm_encounter_count = None\n",
    "    max_norm_encounter_count = None\n",
    "    min_norm_encounter_count = None\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
    "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "#Compute the event count metrics\n",
    "start_time = time.time()\n",
    "event_count = event_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(event_count)\n",
    "\n",
    "#Compute the encounter count metrics\n",
    "start_time = time.time()\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(encounter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "event_count = event_count_metrics(events, hf)\n",
    "assert event_count == (188.9375, 2046, 28, 118.64423076923077, 1014, 6), \"event_count failed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "assert encounter_count == (2.8060810810810812, 34, 2, 2.189423076923077, 11, 1), \"encounter_count failed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature construction\n",
    "\n",
    "It is a common practice to convert raw data into a standard data format before running real machine learning models. In this question, you will implement the necessary python functions in this script. You will work with *events.csv*, *hf_events.csv* and *event_feature_map.csv* files provided in **TRAIN_DATA_PATH** folder. The use of `pandas` library in this question is recommended. \n",
    "- **Index vid**: Index vid is evaluated as follows:\n",
    "  - For heart failure patients: Index vid is the vid of the first visit with heart failure for that patient (i.e., vid field in *hf_events.csv*). \n",
    "  - For normal patients: Index vid is the vid of the last visit for that patient (i.e., vid field in *events.csv*). \n",
    "- **Observation Window**: The time interval you will use to identify relevant events. Only events present in this window should be included while constructing feature vectors.\n",
    "- **Prediction Window**: A fixed time interval that is to be used to make the prediction.\n",
    "\n",
    "In the example above, the index vid is 3. Visits with vid 0, 1, 2 are within the observation window. The prediction window is between visit 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compute the index vid\n",
    "\n",
    "Use the definition provided above to compute the index vid for all patients. Complete the method `read_csv` and `calculate_index_vid` provided in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    Read the events.csv, hf_events.csv and event_feature_map.csv files.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
    "\n",
    "    return events, hf, feature_map\n",
    "\n",
    "\n",
    "def calculate_index_vid(events, hf):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Create list of normal patients (hf_events.csv only contains information about heart failure patients).\n",
    "        2. Split events into two groups based on whether the patient has heart failure or not.\n",
    "        3. Calculate index vid for each patient.\n",
    "    \n",
    "    IMPORTANT:\n",
    "        `indx_vid` should be a pd dataframe with header `['pid', 'indx_vid']`.\n",
    "    '''\n",
    "\n",
    "    indx_vid = ''\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return indx_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "indx_vid_df = calculate_index_vid(events, hf)\n",
    "assert indx_vid_df.shape == (4000, 2), \"calculate_index_vid failed!\"\n",
    "\n",
    "indx_vid = dict(list(zip(indx_vid_df.pid, indx_vid_df.indx_vid)))\n",
    "assert indx_vid[78] == 1, \"calculate_index_vid failed!\"\n",
    "assert indx_vid[1230] == 5, \"calculate_index_vid failed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter events\n",
    "\n",
    "Remove the events that occur outside the observation window. That is, all events in visits before index vid. Complete the method *filter_events* provided in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(events, indx_vid):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Join indx_vid with events on pid.\n",
    "        2. Filter events occuring in the observation window [:, index vid) (Note that the right side is OPEN).\n",
    "    \n",
    "    \n",
    "    IMPORTANT:\n",
    "        `filtered_events` should be a pd dataframe withe header  `['pid', 'event_id', 'value']`.\n",
    "    '''\n",
    "\n",
    "    filtered_events = None\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "indx_vid = calculate_index_vid(events, hf)\n",
    "filtered_events = filter_events(events, indx_vid)\n",
    "assert filtered_events[filtered_events.pid == 78].shape == (128, 3), \"filter_events failed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Aggregate events\n",
    "\n",
    "To create features suitable for machine learning, we will need to aggregate the events for each patient as follows:\n",
    "\n",
    "- **count** occurences for each event.\n",
    "\n",
    "Each event type will become a feature and we will directly use event_id as feature name. For example, given below raw event sequence for a patient,\n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,DIAG_244,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "We can get feature value pairs *(event_id, value)* for this patient with ID *33* as\n",
    "```\n",
    "(DIAG_244, 2.0)\n",
    "(LAB_50971, 1.0)\n",
    "(LAB_50931, 2.0)\n",
    "(DIAG_427, 1.0)\n",
    "(DRUG_0, 1.0)\n",
    "(DRUG_3, 2.0)\n",
    "```\n",
    "\n",
    "Next, replace each *event_id* with the *feature_id* provided in *event_feature_map.csv*.\n",
    "\n",
    "```\n",
    "(146, 2.0)\n",
    "(1434, 1.0)\n",
    "(1429, 2.0)\n",
    "(304, 1.0)\n",
    "(898, 1.0)\n",
    "(1119, 2.0)\n",
    "```\n",
    "\n",
    "Lastly, in machine learning algorithm like logistic regression, it is important to normalize different features into the same scale. We will use the [min-max normalization](http://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range) approach. (Note: we define $min(x)$ is always 0, i.e. the scale equation become $x$/$max(x)$).\n",
    "\n",
    "Complete the method *aggregate_events* provided in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_events(filtered_events_df, hf_df, feature_map_df):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Replace event_id's with index available in event_feature_map.csv.\n",
    "        2. Aggregate events using count to calculate feature value.\n",
    "        3. Normalize the values obtained above using min-max normalization(the min value will be 0 in all scenarios).\n",
    "    \n",
    "    \n",
    "    IMPORTANT:\n",
    "        `aggregated_events` should be a pd dataframe with header `['pid', 'feature_id', 'feature_value']`.\n",
    "    '''\n",
    "    \n",
    "    aggregated_events = None\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return aggregated_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "index_vid = calculate_index_vid(events, hf)\n",
    "filtered_events = filter_events(events, index_vid)\n",
    "aggregated_events = aggregate_events(filtered_events, hf, feature_map)\n",
    "assert aggregated_events[aggregated_events.pid == 88037].shape == (29, 3), \"aggregate_events failed!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save in  SVMLight format\n",
    "\n",
    "If the dimensionality of a feature vector is large but the feature vector is sparse (i.e. it has only a few nonzero elements), sparse representation should be employed. In this problem you will use the provided data for each patient to construct a feature vector and represent the feature vector in SVMLight format.\n",
    "\n",
    "```\n",
    "<line> .=. <target> <feature>:<value> <feature>:<value>\n",
    "<target> .=. 1 | 0\n",
    "<feature> .=. <integer>\n",
    "<value> .=. <float>\n",
    "```\n",
    "\n",
    "The target value and each of the feature/value pairs are separated by a space character. Feature/value pairs MUST be ordered by increasing feature number. **(Please do this in `save_svmlight()`.)** Features with value zero can be skipped. For example, the feature vector in SVMLight format will look like: \n",
    "\n",
    "```\n",
    "1 2:0.5 3:0.12 10:0.9 2000:0.3\n",
    "0 4:1.0 78:0.6 1009:0.2\n",
    "1 33:0.1 34:0.98 1000:0.8 3300:0.2\n",
    "1 34:0.1 389:0.32\n",
    "```\n",
    "\n",
    "where, 1 or 0 will indicate whether the patient has heart failure or not (i.e. the label) and it will be followed by a series of feature-value pairs **sorted** by the feature index (idx) value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import collections\n",
    "\n",
    "def create_features(events_in, hf_in, feature_map_in):\n",
    "\n",
    "    indx_vid = calculate_index_vid(events_in, hf_in)\n",
    "\n",
    "    #Filter events in the observation window\n",
    "    filtered_events = filter_events(events_in, indx_vid)\n",
    "\n",
    "    #Aggregate the event values for each patient \n",
    "    aggregated_events = aggregate_events(filtered_events, hf_in, feature_map_in)\n",
    "\n",
    "    '''\n",
    "    TODO: Complete the code below by creating two dictionaries.\n",
    "        1. patient_features : Key is pid and value is array of tuples(feature_id, feature_value). \n",
    "                              Note that pid should be integer.\n",
    "        2. hf : Key is pid and value is heart failure label.\n",
    "    '''\n",
    "    patient_features = None\n",
    "    hf = None\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return patient_features, hf\n",
    "\n",
    "def save_svmlight(patient_features, hf, op_file):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Create op_file: - which saves the features in svmlight format. (See instructions in section 2.4 for detailed explanatiom)\n",
    "    \n",
    "    Note: Please make sure the features are ordered in ascending order, and patients are stored in ascending order as well.     \n",
    "    To save the files, you could write:\n",
    "        deliverable.write(bytes(f\"{label} {feature_value} \\n\", 'utf-8'))\n",
    "    '''\n",
    "    \n",
    "    deliverable = open(op_file, 'wb')\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "events_in = events_in.loc[:1000]\n",
    "hf_in = hf_in.loc[:100]\n",
    "patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "assert 78 in patient_features, \"create_features is missing patients\"\n",
    "assert len(patient_features[78]) == 127, \"create_features is wrong\"\n",
    "assert patient_features[78][:5] == [(20, 1.0), (164, 1.0), (175, 1.0), (182, 1.0), (190, 1.0)], \"create_features is wrong\"\n",
    "assert len(hf) == 101, \"create_features is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "    save_svmlight(patient_features, hf, 'features_svmlight.train')\n",
    "    \n",
    "    events_in, hf_in, feature_map_in = read_csv(VAL_DATA_PATH)\n",
    "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "    save_svmlight(patient_features, hf, 'features_svmlight.val')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Predictive Modeling \n",
    "\n",
    "Make sure you have finished section 2 before you start to work on this question because some of the files generated in section 2 (*features_svmlight.train*) will be used in this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Creation\n",
    "\n",
    "In the previous question, you constructed feature vectors for patients to be used as training data in various predictive models (classifiers). Now you will use this training data (*features_svmlight.train*) in 3 predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - a. Implement Logistic Regression, SVM and Decision Tree. Skeleton code is provided in the following code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
    "# OR THE TESTS WILL NEVER PASS.\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "#input: X_train, Y_train\n",
    "#output: Y_pred\n",
    "def logistic_regression_pred(X_train, Y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_train. Use default params for the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: X_train, Y_train\n",
    "#output: Y_pred\n",
    "def svm_pred(X_train, Y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a SVM classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_train. Use default params for the classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: X_train, Y_train\n",
    "#output: Y_pred\n",
    "def decisionTree_pred(X_train, Y_train):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_train. Use max_depth as 5.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: Y_pred,Y_true\n",
    "#output: accuracy, precision, recall, f1-score\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Calculate the above mentioned metrics.\n",
    "    NOTE: It is important to provide the output in the same order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: Name of classifier, predicted labels, actual labels\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "\n",
    "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train), Y_train)\n",
    "    display_metrics(\"SVM\",svm_pred(X_train, Y_train),Y_train)\n",
    "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train), Y_train)\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from utils import get_data_from_svmlight\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "### 3.1a Training Accuracy [3 points]\n",
    "X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
    "\n",
    "# test_accuracy_lr\n",
    "expected = 0.856338028169014\n",
    "Y_pred = logistic_regression_pred(X_train, Y_train)\n",
    "actual = classification_metrics(Y_pred, Y_train)[0]\n",
    "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_accuracy_lr failed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - b. Evaluate your predictive models on a separate test dataset in *features_svmlight.val* (binary labels are provided in that svmlight file as the first field). Skeleton code is provided in the following code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
    "# OR THE TESTS WILL NEVER PASS.\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "#input: X_train, Y_train and X_test\n",
    "#output: Y_pred\n",
    "def logistic_regression_pred(X_train, Y_train, X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: train a logistic regression classifier using X_train and Y_train. \n",
    "    Use this to predict labels of X_test use default params for the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "#input: X_train, Y_train and X_test\n",
    "#output: Y_pred\n",
    "def svm_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a SVM classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_test use default params for the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: X_train, Y_train and X_test\n",
    "#output: Y_pred\n",
    "def decisionTree_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_test.\n",
    "    IMPORTANT: use max_depth as 5. Else your test cases might fail.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "#input: Y_pred,Y_true\n",
    "#output: accuracy, precision, recall, f1-score\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Calculate the above mentioned metrics.\n",
    "    NOTE: It is important to provide the output in the same order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: Name of classifier, predicted labels, actual labels\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "    X_test, Y_test = utils.get_data_from_svmlight(os.path.join(\"features_svmlight.val\"))\n",
    "\n",
    "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train, X_test), Y_test)\n",
    "    display_metrics(\"SVM\", svm_pred(X_train, Y_train, X_test), Y_test)\n",
    "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train, X_test), Y_test)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from utils import get_data_from_svmlight\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "### 3.1b Prediction Accuracy [3 points]\n",
    "X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
    "X_test, Y_test = get_data_from_svmlight(\"features_svmlight.val\")\n",
    "\n",
    "# test_accuracy_lr\n",
    "expected = 0.6937086092715232\n",
    "Y_pred = logistic_regression_pred(X_train, Y_train, X_test)\n",
    "actual = classification_metrics(Y_pred, Y_test)[0]\n",
    "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_accuracy_lr failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Validation\n",
    "\n",
    "In order to fully utilize the available data and obtain more reliable results, machine learning practitioners use cross-validation to evaluate and improve their predictive models. You will demonstrate using two cross-validation strategies against SVD. \n",
    "\n",
    "- K-fold: Divide all the data into $k$ groups of samples. Each time $\\frac{1}{k}$ samples will be used as test data and the remaining samples as training data.\n",
    "- Randomized K-fold: Iteratively random shuffle the whole dataset and use top specific percentage of data as training and the rest as test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the two cross-validation strategies.**\n",
    "- **K-fold:** Use the number of iterations k=5; \n",
    "- **Randomized K-fold**: Use a test data percentage of 20\\% and k=5 for the number of iterations for Randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from numpy import mean\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
    "# OR THE TESTS WILL NEVER PASS.\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "#input: training data and corresponding labels\n",
    "#output: accuracy, f1\n",
    "def get_f1_kfold(X, Y, k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: First get the train indices and test indices for each iteration.\n",
    "    Then train the classifier accordingly.\n",
    "    Report the mean f1 score of all the folds.\n",
    "    \n",
    "    Note that you do not need to set random_state for KFold, as it has no effect since shuffle is False by default.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "#input: training data and corresponding labels\n",
    "#output: accuracy, f1\n",
    "def get_f1_randomisedCV(X, Y, iterNo=5, test_percent=0.20):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: First get the train indices and test indices for each iteration.\n",
    "    Then train the classifier accordingly.\n",
    "    Report the mean f1 score of all the iterations.\n",
    "    \n",
    "    Note that you need to set random_state for ShuffleSplit\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X,Y = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "    print(\"Classifier: SVD\")\n",
    "    f1_k = get_f1_kfold(X,Y)\n",
    "    print((\"Average F1 Score in KFold CV: \"+str(f1_k)))\n",
    "    f1_r = get_f1_randomisedCV(X,Y)\n",
    "    print((\"Average F1 Score in Randomised CV: \"+str(f1_r)))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QA CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "### 3.2 Cross Validation F1 [10 points]\n",
    "# test_f1_cv_kfold\n",
    "expected = 0.7258461959533061\n",
    "X, Y = get_data_from_svmlight(\"features_svmlight.train\")\n",
    "actual = get_f1_kfold(X, Y)\n",
    "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_f1_cv_kfold failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
